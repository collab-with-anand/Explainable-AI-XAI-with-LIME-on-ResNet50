# ğŸ§ ğŸ” Explainable AI with LIME â€“ ResNet50 Interpretability


## ğŸŒŸ Project Overview

This project explores **Explainable AI (XAI)** using **LIME** (Local Interpretable Model-Agnostic Explanations) to interpret predictions made by a **ResNet50** deep-learning model pretrained on ImageNet.
It visualizes *which regions of an image most influence the modelâ€™s decision*, providing transparency into how convolutional networks â€œseeâ€ objects.

---

## ğŸ§  Objective

* âœ… Use **ResNet50** to generate class predictions on sample images.
* ğŸ§© Apply **LIME** to create pixel-level visual explanations of model reasoning.
* ğŸ¨ Identify **positively** (supportive) and **negatively** (contradictory) influential image regions.

---

## ğŸ§© Pipeline Summary

|         ğŸ”¢ Step         | ğŸ“˜ Description                                                              |
| :---------------------: | --------------------------------------------------------------------------- |
|   **1. Model Loading**  | Load pretrained ResNet50 weights (ImageNet).                                |
|   **2. Preprocessing**  | Resize â†’ Normalize â†’ Batch images for prediction.                           |
|    **3. Prediction**    | Obtain top-5 classes and probabilities.                                     |
| **4. LIME Explanation** | Perturb superpixels â†’ fit local surrogate model â†’ estimate feature weights. |
|   **5. Visualization**  | Overlay superpixels showing positive (green) and negative (red) influences. |
|  **6. Interpretation**  | Discuss confidence scores and relevance of regions.                         |

---

## ğŸ—ï¸ Model + Explainability Framework

```text
Input Image
   â†“
ResNet50 (Pretrained on ImageNet)
   â†“
Predicted Class + Probability
   â†“
LIME Explainer (Perturbations + Local Surrogate Model)
   â†“
Visualization â†’ Positive (green) / Negative (red) Influence
```

---

## ğŸ’¡ Key Idea

While ResNet50 is highly accurate, it behaves as a **black box**.
LIME builds a simplified, interpretable local model to reveal *why* the network made a specific prediction.
By perturbing parts of an image and observing changes in prediction confidence, we learn **which regions truly drive model behavior**.

---

## ğŸ“Š Results

| ğŸ¤ª Metric                          |                                ğŸ”¢ Result                               |
| :--------------------------------- | :--------------------------------------------------------------------: |
| **Top-1 Accuracy (sample subset)** |                                 â‰ˆ 78 %                                 |
| **Explanation Time (per image)**   |                              3 â€“ 5 seconds                             |
| **Visualization Clarity**          |           Accurately highlights discriminative object regions          |
| **Observation**                    | Model attention aligns with true object; background contributes weakly |

**Insights:**

* LIME explanations consistently match regions containing the actual object.
* Some misclassifications occur when background textures dominate feature maps.
* Visual inspection confirms LIMEâ€™s reliability for interpreting CNN confidence.

---

## ğŸ–¼ï¸ Sample Output

| Input Image                        | Positive Regions (Support Prediction) |  Negative Regions (Oppose Prediction) |
| :--------------------------------- | :-----------------------------------: | :-----------------------------------: |
| Rhodesian Ridgeback | ![Positive](images/LIME_Positive.png) | ![Negative](images/LIME_Negative.png) |

**Interpretation:**
ğŸŸ¢ *Green regions* â†’ increase confidence in predicted class.
ğŸ”´ *Red regions* â†’ reduce confidence or confuse the model.
Example: In a dog image, LIME highlights the **head and torso** (green) while background grass or sky (red) has little or negative effect.

---

## ğŸ” Key Takeaways

* ğŸ’¡ **LIME** reveals what drives model confidence â€” bridging the gap between black-box CNNs and human understanding.
* ğŸ§  Superpixel-level analysis provides interpretable visuals of feature importance.
* âš¡ Explainability is crucial for **trust, fairness, and AI accountability**.
* ğŸ”¬ CNNs often leverage textures and context more than shape â€” LIME exposes these patterns clearly.

---

## âš™ï¸ Environment Setup

```bash
git clone https://github.com/<your-username>/explainable-ai-lime.git
cd explainable-ai-lime
pip install -r requirements.txt
```

### ğŸ§© Dependencies

* tensorflow / keras
* lime
* numpy, matplotlib, opencv-python

